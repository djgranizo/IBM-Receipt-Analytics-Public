{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Receipt Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Image-to-Text - Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Microsoft Azure Subsription key and corresponding server\n",
    "- URL of Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the word bounding boxes and text.\n",
    "# Returns an Array That Contains a Dictionary with Bounding Box\n",
    "# Coordinates and the Text Labels\n",
    "def extract_word_info(line_infos):\n",
    "    word_infos = []\n",
    "    for line in line_infos:\n",
    "        for word_metadata in line:\n",
    "            for word_info in word_metadata[\"words\"]:\n",
    "                word_infos.append(word_info)\n",
    "    return word_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Output\n",
    "# Saves a Text File That Contains Information of the Word Bounding Boxes and Text Labels\n",
    "def save_output(word_infos, file_name=\"output\"):\n",
    "    words = []\n",
    "    for i in word_infos: words.append(str(i))\n",
    "    with open(\"text/\" + file_name + \".txt\", \"w\") as f:\n",
    "        for line in words:\n",
    "            f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Json Output\n",
    "# Saves a Text File that contains the Json Output from Microsoft Azure\n",
    "def save_json(json_data, file_name=\"json\"):\n",
    "    #print(json_data)\n",
    "    type(json_data)\n",
    "    with open('json/' + file_name + '.txt', 'w') as outfile:\n",
    "        outfile.write(json.dumps(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the image and overlay it with the extracted text.\n",
    "def create_image(image_url, word_infos, save=False, img_name=\"img\"):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "    ax = plt.imshow(image, alpha=0.5)\n",
    "    for word in word_infos:\n",
    "        bbox = [int(num) for num in word[\"boundingBox\"].split(\",\")]\n",
    "        text = word[\"text\"]\n",
    "        origin = (bbox[0], bbox[1])\n",
    "        patch  = Rectangle(origin, bbox[2], bbox[3], fill=False, linewidth=2, color='y')\n",
    "        ax.axes.add_patch(patch)\n",
    "        plt.text(origin[0], origin[1], text, fontsize=20, weight=\"bold\", va=\"top\")\n",
    "    plt.axis(\"off\")\n",
    "    if save: plt.savefig('img/' + img_name + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR MICROSOFT SUBSCRIPTION KEY\n",
    "subscription_key = \"af3d8afcd1a64257aeed676d27958a8a\"\n",
    "image_url = \"\"\n",
    "\n",
    "def receipt_ocr(img_id, sub_key=subscription_key, save=True):\n",
    "    img_id = str(img_id)\n",
    "    print(\"Currently Processing: \" + img_id + \".jpg\")\n",
    "    # Setup Key and Vision Base URL\n",
    "    subscription_key = sub_key\n",
    "    assert subscription_key\n",
    "    vision_base_url = \"https://eastus.api.cognitive.microsoft.com/vision/v2.0/\"\n",
    "    ocr_url = vision_base_url + \"ocr\"\n",
    "    # Set up API Request Parameters\n",
    "    image_url = \"http://kunalsharma.net/reciept_project/\" + img_id + \".jpg\"\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "    params  = {'language': 'unk', 'detectOrientation': 'true'}\n",
    "    data    = {'url': image_url}\n",
    "    response = requests.post(ocr_url, headers=headers, params=params, json=data)\n",
    "    response.raise_for_status()\n",
    "    analysis = response.json()\n",
    "    save_json(analysis, img_id)\n",
    "    \n",
    "    # Extract Word Information form JSON\n",
    "    line_infos = [region[\"lines\"] for region in analysis[\"regions\"]]\n",
    "    word_infos = extract_word_info(line_infos)\n",
    "    # Save a Txt File With Word Info\n",
    "    save_output(word_infos, img_id)\n",
    "    # Save the Annotated Image\n",
    "    create_image(image_url, word_infos, save=save, img_name=img_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spacy\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# Prereq: python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/DJ/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/DJ/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Formatting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Punctuation\n",
    "# param text (arr) - Text to Clean\n",
    "def remove_punc(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# Remove Digits\n",
    "# param text (str) - Text to clean\n",
    "def remove_digit(text):\n",
    "    return re.sub('\\d+', '',text)\n",
    "\n",
    "# Remove Words that are Numbers from a List\n",
    "# param text (arr) - List of Words\n",
    "def remove_num(text):\n",
    "    return [w for w in text if not w.isdigit()]\n",
    "\n",
    "# Tokenize Words\n",
    "# param text (arr) - List of Words\n",
    "def tokenize_words(text):\n",
    "    return word_tokenize(' '.join(text))\n",
    "\n",
    "# Remove Stop Words\n",
    "# param text (arr) - List of Words\n",
    "def remove_stop(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in text if not w in stop_words]\n",
    "\n",
    "# Remove Custom Stop Words\n",
    "# param text (arr) - List of Words\n",
    "def remove_custom_stop(text):\n",
    "    custom_stop_words = ['visit', 'back', 'visa', 'change', 'thanks', 'survey', 'total', 'tax',\n",
    "                         'coupon', 'invalid', 'details', 'your', 'emp', 'rewards', 'please',\n",
    "                         'see', 'register', 'order', 'take', 'again', 'our', 'subtotal', 'for',\n",
    "                         'rebate', 'cash', 'website', 'id', 'come', 'tendered', 'building', 'street']\n",
    "    return [w for w in text if not w in custom_stop_words]\n",
    "\n",
    "# Filter for only Nouns\n",
    "# param text (arr) - List of Words\n",
    "def remove_non_nouns(text):\n",
    "    return [w for w in text if nlp(w)[0].pos_ == 'NOUN']\n",
    "\n",
    "# Remove Meaningless Words\n",
    "# param text (arr) - List of Words\n",
    "# param report (bool) - Print Number of Removed Words\n",
    "def remove_meaningless(text, report=True):     \n",
    "    cleaned = [w for w in text if not nlp(w).vector_norm == 0]\n",
    "#     if report: print('Removed ' + str(len(text) - len(cleaned)) + ' word(s) out of ' + str(len(text)))\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "# Extrats Text from the Formatted Text Output of Azure Text Detection\n",
    "# param text_id (str) - ID of the text (index)\n",
    "def extract_text(text_id):\n",
    "    file = open('text/' + str(text_id) +'.txt', 'r')\n",
    "    text = file.readlines()\n",
    "    words = []\n",
    "    for i in text:\n",
    "        word = i.split(':')[-1].split('}')[0].strip()\n",
    "        word = remove_punc(word)\n",
    "        if word != '': words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "# Extract and Clean Text\n",
    "# param text_id (str) - ID of the text\n",
    "def format_text(text_id):\n",
    "    text = extract_text(text_id)\n",
    "#     print(text)\n",
    "    text = remove_num(text)\n",
    "    text = tokenize_words(text)\n",
    "    text = remove_stop(text)\n",
    "    text = remove_custom_stop(text)\n",
    "    text = remove_meaningless(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expense Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_types = ['Airfare', 'Car Rental', 'Hotel', 'Fuel', 'Parking', 'Taxi', 'Toll', 'Train', 'Maintenance',\n",
    "                 'Breakfast', 'Meal', 'Dinner', 'Lunch', 'Groceries', 'Coffee', 'Entertainment', 'Office Supplies', 'Software',\n",
    "                 'Online Fees', 'Mobile', 'Cellular Phone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorization by Average Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Avg Vector of Text\n",
    "# param text (arr) - Text to Avg\n",
    "# return avg_vect (arr) - Averaged Vector of Text\n",
    "def avg_vect(text):\n",
    "    return nlp(' '.join(text))\n",
    "\n",
    "# Finds the Most Similar Expense Type\n",
    "# param types (arr) - List of Expense Types\n",
    "# param text (arr) - Array of Text\n",
    "def most_similar_by_avg(types, text):\n",
    "    avg_vector = avg_vect(text)\n",
    "    closest_type = ''\n",
    "    closest_score = 0\n",
    "    for i in types:\n",
    "        #print(str(round(avg_vector.similarity(nlp(i)), 2)) + ' ' + i)\n",
    "        similarity = avg_vector.similarity(nlp(i))\n",
    "        if(similarity > closest_score):\n",
    "            closest_score = similarity\n",
    "            closest_type = i\n",
    "    closest_score = round(closest_score * 100, 3)\n",
    "    return closest_type, closest_score  \n",
    "\n",
    "#\n",
    "\n",
    "# Plots Type Similarity Using the Avg Word Vector\n",
    "# types (arr) - List of Expense Types\n",
    "def plot_similar_by_avg(types, text, text_id=''):\n",
    "    avg_vector = avg_vect(text)\n",
    "    sim = []\n",
    "    for i in types: sim.append(avg_vector.similarity(nlp(i)))\n",
    "    sns.barplot(types, sim, alpha=0.9).set_xticklabels(types, rotation=90)\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.title('Similarities by Expense Types for Receipt '+ str(text_id))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "import json\n",
    "import re\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Given string of text extracted from the receipt, Identify the date of Purchase\n",
    "# Returns the First Date Object Found\n",
    "# param text (arr) - List of Words\n",
    "def date_find(text):\n",
    "    sentence = ' '.join(text)\n",
    "    matches = datefinder.find_dates(sentence)\n",
    "    dates = [i for i in matches]\n",
    "    if(len(dates) == 0): \n",
    "        return None\n",
    "    else: \n",
    "        return dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import copy\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "# param text (arr) - Text to Clean\n",
    "def remove_punc(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# Remove Digits\n",
    "# param text (str) - Text to clean\n",
    "def remove_digit(text):\n",
    "    return re.sub('\\d+', '',text)\n",
    "\n",
    "\n",
    "# Given an Analysis (JSON) output from Azure OCR, output the lines in order\n",
    "# param: text id (int) - repecetive text id for receipt\n",
    "def extract_lines(text_id):\n",
    "    with open(\"json/\" + str(text_id) + \".txt\",\"r\") as fin:\n",
    "        analysis = json.load(fin)\n",
    "    \n",
    "    unsorted_lines = []\n",
    "    for region in analysis[\"regions\"]:\n",
    "        for line in region[\"lines\"]:\n",
    "            unsorted_lines.append(line)\n",
    "               \n",
    "    # sort by Y coordinates\n",
    "    sorted_lines = sorted(unsorted_lines, key = lambda x: int(x[\"boundingBox\"].split(\",\")[1]))\n",
    "    \n",
    "    return sorted_lines\n",
    "        \n",
    "# Merge word lists in line dictionaries.\n",
    "# Used to merge lines that were not identified in the analysis json\n",
    "# param: line1 - line to merge\n",
    "# param: line2 - line to merge\n",
    "def merge_words(line1,line2):\n",
    "    out_line = line1\n",
    "    out_line[\"words\"] += line2[\"words\"]\n",
    "    return out_line\n",
    "    \n",
    "# Iterate through sorted lines and merging lines that are found to be separate\n",
    "# Lines will be merged if the top edge of the text is within the defined slack\n",
    "# This is used to identify lines that could be angled or shifted as seen below\n",
    "#\n",
    "# Total:\n",
    "#                50.00\n",
    "#\n",
    "# param: sorted_lines (list) - sorted lines output from extract lines\n",
    "# param: slack (int) - the number of pixels above and below the top edge of text \n",
    "#        that will be considered for merging lines\n",
    "def merge_lines(sorted_lines, slack=None):\n",
    "    merged_lines = []\n",
    "    for line_i in sorted_lines:\n",
    "        for line_j in sorted_lines:\n",
    "            line_i_BBy = int(line_i[\"boundingBox\"].split(\",\")[1])\n",
    "            line_j_BBy = int(line_j[\"boundingBox\"].split(\",\")[1])\n",
    "            line_i_BBx = int(line_i[\"boundingBox\"].split(\",\")[0])\n",
    "            line_j_BBx = int(line_j[\"boundingBox\"].split(\",\")[0])\n",
    "            \n",
    "            if slack == None:\n",
    "                slack = int(line_i[\"boundingBox\"].split(\",\")[3]) / 4\n",
    "            \n",
    "            if abs(line_i_BBy - line_j_BBy) < slack and line_i != line_j and line_i_BBx < line_j_BBx:\n",
    "                merge_words(line_i, line_j)\n",
    "                sorted_lines.remove(line_j)\n",
    "                \n",
    "        merged_lines.append(line_i)\n",
    "    return merged_lines\n",
    " \n",
    "# Condensed pipeline of functions to output a final list of text lines\n",
    "# param text_id (int) - id for the desired analysis JSON file\n",
    "def get_lines(text_id):\n",
    "    sorted_lines = extract_lines(text_id)\n",
    "    output = merge_lines(sorted_lines)\n",
    "    return output\n",
    "\n",
    "# Given a line dictionary, iterate through the words for the first number\n",
    "# param line (dict) line dictionary containing word dict\n",
    "def num_search(line):\n",
    "    for word in line[\"words\"]:\n",
    "        test_word = word[\"text\"].replace(\"$\",\"\")\n",
    "        try:\n",
    "            total = float(test_word)\n",
    "            return total\n",
    "        except:\n",
    "            #print(\"searching\")\n",
    "            pass\n",
    "            \n",
    "    #print(\"Could not find total\")\n",
    "    return None\n",
    "\n",
    "# Given a line dictionary, iterate searching for a word similar to \"total\"\n",
    "# param: line (dict) - line dictionary containing words\n",
    "def string_search(line, similarity=0.7):\n",
    "    for word in line[\"words\"]:\n",
    "        test_word = remove_punc(remove_digit(word[\"text\"].lower()))\n",
    "        if ratio(test_word, \"total\") > similarity:\n",
    "            #print(word[\"text\"], test_word, ratio(test_word,\"total\"))\n",
    "            return True\n",
    "        \n",
    "# Final pipeline, Given a text id, collect lines and serach for total amount\n",
    "# param text_id (int) - the id for the json file\n",
    "def extract_total(text_id):\n",
    "    for line in get_lines(text_id)[::-1]:\n",
    "        if string_search(line):\n",
    "            total = num_search(line)\n",
    "            if total is not None:\n",
    "                return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEverything(text_id):\n",
    "    text = format_text(text_id)\n",
    "    dateFound = date_find(text)\n",
    "    totalFound = extract_total(text_id)\n",
    "    expenseType, dummy = most_similar_by_avg(expense_types, text)\n",
    "    return  text_id , totalFound, dateFound, expenseType"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
